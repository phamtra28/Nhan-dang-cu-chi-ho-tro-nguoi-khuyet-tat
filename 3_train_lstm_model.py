import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split

# Load d·ªØ li·ªáu
X = np.load('gesture_data/X.npy')
y = np.load('gesture_data/y.npy')
action_list = np.load('gesture_data/action_map.npy', allow_pickle=True)

print("\nüìä Th√¥ng tin dataset:")
print(f"- S·ªë m·∫´u hu·∫•n luy·ªán: {X.shape[0]}")
print(f"- S·ªë c·ª≠ ch·ªâ: {len(action_list)}")
print(f"- C√°c c·ª≠ ch·ªâ: {', '.join(action_list)}")

# Chia d·ªØ li·ªáu th√†nh train-val-test (60-20-20)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print("\nüî¢ Ph√¢n chia d·ªØ li·ªáu:")
print(f"- Train: {X_train.shape[0]} m·∫´u")
print(f"- Validation: {X_val.shape[0]} m·∫´u")
print(f"- Test: {X_test.shape[0]} m·∫´u")

# T·∫°o m√¥ h√¨nh LSTM c·∫£i ti·∫øn
model = Sequential([
    Input(shape=(30, 1662)),
    BatchNormalization(),
    LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    LSTM(256, return_sequences=True, kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    LSTM(128),
    Dropout(0.3),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dense(len(action_list), activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', 'top_k_categorical_accuracy']
)

print("\nüß† Th√¥ng tin m√¥ h√¨nh:")
model.summary()

# Callbacks
callbacks = [
    ModelCheckpoint(
        'best_model.keras',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=0.00001,
        verbose=1
    )
]

# Hu·∫•n luy·ªán m√¥ h√¨nh
print("\nüèãÔ∏è B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=16,
    validation_data=(X_val, y_val),
    callbacks=callbacks,
    verbose=1
)

# ƒê√°nh gi√° tr√™n t·∫≠p test
print("\nüß™ ƒê√°nh gi√° tr√™n t·∫≠p test:")
test_loss, test_acc, test_top_k = model.evaluate(X_test, y_test, verbose=0)
print(f"- Test Accuracy: {test_acc:.4f}")
print(f"- Test Top-3 Accuracy: {test_top_k:.4f}")
print(f"- Test Loss: {test_loss:.4f}")

# L∆∞u m√¥ h√¨nh cu·ªëi c√πng
model.save("final_model.keras")
print("\n‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh th√†nh c√¥ng:")
print("- best_model.keras (m√¥ h√¨nh t·ªët nh·∫•t tr√™n validation)")
print("- final_model.keras (m√¥ h√¨nh sau khi hu·∫•n luy·ªán xong)")
