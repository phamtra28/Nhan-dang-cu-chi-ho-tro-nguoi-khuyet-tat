import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model

# T·∫£i m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√† danh s√°ch c√°c c·ª≠ ch·ªâ
try:
    model = load_model('D:/H·ªçcT·∫≠p/Nam3-Ki1/New folder/BTL-CHUYENDOISO/best_model.keras')
    action_list = np.load('gesture_data/action_map.npy', allow_pickle=True)
except FileNotFoundError:
    print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y 'final_model.keras' ho·∫∑c 'gesture_data/action_map.npy'.")
    print("Vui l√≤ng ƒë·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y th√†nh c√¥ng ƒëo·∫°n m√£ hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥.")
    exit()

# C·∫•u h√¨nh MediaPipe
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def mediapipe_detection(image, model):
    """X·ª≠ l√Ω ·∫£nh ƒë·ªÉ ph√°t hi·ªán c√°c ƒëi·ªÉm m·ªëc c·ªßa t∆∞ th·∫ø, khu√¥n m·∫∑t v√† tay."""
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def extract_keypoints(results):
    """Tr√≠ch xu·∫•t m·ªôt m·∫£ng ph·∫≥ng c√°c keypoints t·ª´ k·∫øt qu·∫£ c·ªßa MediaPipe."""
    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, face, lh, rh])

def draw_styled_landmarks(image, results):
    """V·∫Ω c√°c ƒëi·ªÉm m·ªëc v·ªõi m√†u s·∫Øc v√† ki·ªÉu d√°ng t√πy ch·ªânh."""
    # V·∫Ω c√°c k·∫øt n·ªëi c·ªßa pose
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), 
                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)) 
    # V·∫Ω c√°c k·∫øt n·ªëi c·ªßa khu√¥n m·∫∑t
    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, 
                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), 
                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)) 
    # V·∫Ω c√°c k·∫øt n·ªëi c·ªßa tay tr√°i
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), 
                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)) 
    # V·∫Ω c√°c k·∫øt n·ªëi c·ªßa tay ph·∫£i
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), 
                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)) 

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh nh·∫≠n di·ªán c·ª≠ ch·ªâ theo th·ªùi gian th·ª±c."""
    sequence = []
    sentence = []
    predictions = []
    
    # B·∫°n c√≥ th·ªÉ thay ƒë·ªïi gi√° tr·ªã n√†y ƒë·ªÉ ƒëi·ªÅu ch·ªânh ƒë·ªô nh·∫°y
    threshold = 0.1 
    
    cooldown_frames = 15 
    cooldown_counter = 0
    SEQUENCE_LENGTH = 30 

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("‚ùå L·ªói: Kh√¥ng th·ªÉ m·ªü webcam.")
        return

    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        print("üí° B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán c·ª≠ ch·ªâ theo th·ªùi gian th·ª±c. Nh·∫•n 'q' ƒë·ªÉ tho√°t.")
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            image, results = mediapipe_detection(frame, holistic)
            draw_styled_landmarks(image, results)
            
            keypoints = extract_keypoints(results)
            sequence.append(keypoints)
            sequence = sequence[-SEQUENCE_LENGTH:] 

            if len(sequence) == SEQUENCE_LENGTH:
                print(f"Chu·ªói ƒë·ªß {SEQUENCE_LENGTH} khung h√¨nh. ƒêang d·ª± ƒëo√°n...")
                
                if cooldown_counter == 0:
                    res = model.predict(np.expand_dims(sequence, axis=0))[0]
                    predictions.append(np.argmax(res))
                    
                    print("C√°c d·ª± ƒëo√°n:", res)
                    print("D·ª± ƒëo√°n cao nh·∫•t:", action_list[np.argmax(res)], f"v·ªõi ƒë·ªô tin c·∫≠y {res[np.argmax(res)]:.2f}")

                    if res[np.argmax(res)] > threshold:
                        predicted_action = action_list[np.argmax(res)]
                        confidence = res[np.argmax(res)]
                        
                        if len(sentence) > 0 and predicted_action != sentence[-1]:
                            sentence = [predicted_action]
                        elif not sentence:
                            sentence = [predicted_action]
                        
                        cooldown_counter = cooldown_frames
                    
                else:
                    cooldown_counter -= 1
            
            display_text = ""
            confidence_text = ""
            if len(sequence) == SEQUENCE_LENGTH:
                # L·∫•y d·ª± ƒëo√°n m·ªõi nh·∫•t
                res = model.predict(np.expand_dims(sequence, axis=0))[0]
                confidence = res[np.argmax(res)]
                
                display_text = f"Cu chi: {action_list[np.argmax(res)]}"
                confidence_text = f"Do tin cay: {confidence:.2f}"
            else:
                display_text = "Dang thu thap du lieu..."
                
            cv2.putText(image, display_text, (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
            cv2.putText(image, confidence_text, (10, 70),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1, cv2.LINE_AA)

            cv2.imshow('Nhan dien cu chi', image)

            if cv2.waitKey(10) & 0xFF == ord('q'):
                break

    cap.release()
    cv2.destroyAllWindows()
    print("üëã ƒê√£ tho√°t ch∆∞∆°ng tr√¨nh. T·∫°m bi·ªát!")

if __name__ == '__main__':
    main()
